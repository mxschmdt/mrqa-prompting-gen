from typing import Any, Dict, Optional, Union

import openprompt
import torch
from openprompt.data_utils import InputFeatures
from openprompt.utils import signature
from openprompt.utils.logging import logger
from transformers import BartPretrainedModel, GenerationMixin


class PromptForGeneration(openprompt.PromptForGeneration):
    """This class fixes the generation part for newer versions of the transformers library."""

    def _validate_model_kwargs(self, model_kwargs: Dict[str, Any]):
        """Ignore some arguments specific for Prompting for validation"""
        # Excludes arguments that are used by Prompting model but not removed before fed to generate()
        for key in ["attention_mask", "soft_token_ids", "loss_ids"]:
            model_kwargs.pop(key, None)

        return super()._validate_model_kwargs(model_kwargs)

    def can_generate(self):
        # confirms that the model is compatible with generation
        return True

    def _prepare_encoder_decoder_kwargs_for_generation(
        self,
        input_ids: torch.LongTensor,
        model_kwargs,
        model_input_name: Optional[str] = None,
    ) -> Dict[str, Any]:
        if "encoder_outputs" not in model_kwargs:
            if (
                isinstance(self.plm, BartPretrainedModel)
                and "use_cache" in model_kwargs
            ):
                model_kwargs = dict(model_kwargs)
                del model_kwargs["use_cache"]
            else:
                # make sure that use_case is False for the encoder and do not modify dict in-place
                model_kwargs = dict(model_kwargs, use_cache=False)
        return super()._prepare_encoder_decoder_kwargs_for_generation(
            input_ids, model_kwargs, model_input_name
        )

    def _update_model_kwargs_for_generation(
        self, outputs, model_kwargs: Dict[str, Any], **kwargs
    ) -> Dict[str, Any]:
        r"""The parents class's ``_update_model_kwargs_for_generation`` method will
        add ``past_key_values`` to model_kwargs, and update ``token_type_ids``, and ``attention_mask_ids``.

        In case some of the model_kwargs are modified in the prepare_inputs_for_generation function
        and should be used as the subsequent model_kwargs, we update these kwargs before the parent class
        call.

        Other updates should be added here after the parent's function call.

        Fixes the function from the super class.

        Args:
            outputs (:obj:`torch.Tensor`):
            is_encoder_decoder (:obj:`bool`, defaults to False):
        """
        if self.generate_ith_token == 0:
            for key in self.last_model_inputs:
                if key in model_kwargs:
                    model_kwargs[key] = self.last_model_inputs[key]
        model_kwargs = super(
            openprompt.PromptForGeneration, self
        )._update_model_kwargs_for_generation(
            outputs=outputs, model_kwargs=model_kwargs, **kwargs
        )
        self.generate_ith_token += 1
        return model_kwargs

    def generate(
        self,
        batch: Union[Dict, InputFeatures],
        verbose: Optional[bool] = False,
        **generation_kwargs,
    ):
        r"""This function wraps the generate() methods in parent class ``GenerationMixin``.
        Forward uses the ``PretrainedModel``'s forward method.
        generation_kwargs include all the parameters that are passed in to
        ``transformers.generation_util.GenerationMixin.generate``

        Args:
            batch (:obj:`Union[Dict, InputFeatures]`): The input features of batchified data sequences.
            verbose (:obj:`Optional[bool]`): Set to true to verbose the generated sentence.

        Returns:
            output_sequences (:obj:`List[torch.Tensor]`): The raw sequences generated by the generation model.
            generated_sentences (:obj:`List[torch.Tensor]`): The generated sentences that have been post-processed.
        """
        input_generation_kwargs = {
            key: value
            for key, value in generation_kwargs.items()
            if key in signature(GenerationMixin.generate).args
        }
        if self.config.is_encoder_decoder:
            loss_ids_start = batch["loss_ids"].argmax(dim=-1)
            assert (
                loss_ids_start.min() == loss_ids_start.max()
            ), "The generation start from different position in a batch."
            batch["decoder_input_ids"] = batch["decoder_input_ids"][
                :, : loss_ids_start.min() + 1
            ]
            input_length = batch["decoder_input_ids"].size(1)
            batch_size = batch["decoder_input_ids"].size(0)

            self.generate_ith_token = 0
            self.in_generation_function = True
            output_dict = GenerationMixin.generate(
                self,
                **batch,
                **input_generation_kwargs,
                pad_token_id=self.tokenizer.pad_token_id,
                eos_token_id=self.tokenizer.eos_token_id,
                output_scores=True,
                return_dict_in_generate=True,
            )
            output_sequences = output_dict.sequences
            output_scores = output_dict.scores
            self.in_generation_function = False
            output_sequences = output_sequences.cpu().tolist()
            generated_sentences = self.post_processing(
                output_sequences=output_sequences, input_lengths=input_length
            )
        else:
            input_length = batch["input_ids"].size(1)
            batch_size = batch["input_ids"].size(0)

            # Currently huggingface transformers only support single sample generation, or padding to the left (instead of the right).
            # because it will only extract the last position of the output
            # generate one_by_one
            if "input_ids_len" in batch:
                input_real_lens = batch["input_ids_len"]
            else:
                input_real_lens = torch.sum(
                    (batch["input_ids"] != self.tokenizer.pad_token_id).to(torch.int),
                    dim=-1,
                )
            output_sequences = []
            for instance_id in range(batch_size):
                # remove the pad token
                instance = {
                    key: batch[key][instance_id : instance_id + 1][
                        :, : input_real_lens[instance_id]
                    ]
                    for key in batch
                    if isinstance(batch[key], torch.Tensor)
                    and batch[key].shape[:2] == torch.Size([batch_size, input_length])
                }
                self.generate_ith_token = 0
                self.in_generation_function = True
                output_dict = GenerationMixin.generate(
                    self,
                    **instance,
                    **input_generation_kwargs,
                    pad_token_id=self.tokenizer.pad_token_id,
                    eos_token_id=self.tokenizer.eos_token_id,
                    output_scores=True,
                    return_dict_in_generate=True,
                )
                output_sequence = output_dict.sequences
                output_scores = output_dict.scores
                self.in_generation_function = False
                output_sequences.extend(
                    output_sequence.cpu().tolist()
                )  # TODO: to support generate multiple sentence
            generated_sentences = self.post_processing(
                output_sequences=output_sequences,
                input_lengths=input_real_lens.cpu().tolist(),
            )
        if verbose:
            logger.info(f"Generated:{generated_sentences}")
        return output_sequences, generated_sentences, output_scores
